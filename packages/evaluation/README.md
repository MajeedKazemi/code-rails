# Evaluation Test Suite

This evaluation suite has a couple functionalities.

## Prerequisites

1. `LANGCHAIN_API_KEY` environment variable must be set to the API key for the LangSmith API.
2. `python3 -m venv venv` to create a virtual environment.
3. `source venv/bin/activate` to activate the virtual environment.
4. `pip install -r requirements.txt` to install the required Python packages.
5. `yarn` to install the required Node packages.

## LangSmith Dataset Generation

### Story Generation

1. `yarn generate` will generate a dataset of story titles and other associated metadata for use in generating stories for further evaluation.
2. This dataset will be stored available via the associated LangSmith account,

### Solution Generation

1. `python eval.py solutions` will generate a dataset of solutions for the stories previously generated.
2. This will produce three datasets corresponding to `correct`, `incorrect`, and `approximate` solutions for the custom tasks.

## Evaluation

### Story Generation Evaluation

1. Run `python eval.py stories` to evaluate the generated stories.

Node: the dataset from `yarn generate` must be available in the LangSmith account for this to work.

### Feedback Evaluation

1. Run `python eval.py feedback-${correctness}-${feedback-level}` to evaluate the feedback generated by the model.
2. `${correctness}` can be `correct`, `incorrect`, or `approximate` and corresponds to which solution dataset to evaluate feedback on.
3. `${feedback-level}` can be `l1`, `l2`, or `l3` and corresponds to the level of feedback to evaluate.
